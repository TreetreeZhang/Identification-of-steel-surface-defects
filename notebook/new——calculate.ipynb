{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb98f3b5-fe64-4861-b7ae-4ceaea6dacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# Mish 激活函数：一种光滑的非单调激活函数\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "# 多层感知机 (MLP)：包含两个全连接层和Mish激活函数\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=Mish, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features // 4\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)  # 第一个全连接层\n",
    "        self.act = act_layer()  # 激活函数\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)  # 第二个全连接层\n",
    "        self.drop = nn.Dropout(drop)  # Dropout 防止过拟合\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "# 实现相同填充的函数：根据卷积核尺寸和步长，计算需要的填充量，确保输出的尺寸和输入一致\n",
    "def Apply_padding(images, ksizes, strides, rates):\n",
    "    assert len(images.size()) == 4\n",
    "    batch_size, channel, rows, cols = images.size()\n",
    "\n",
    "    # 计算输出尺寸\n",
    "    out_rows = (rows + strides[0] - 1) // strides[0]\n",
    "    out_cols = (cols + strides[1] - 1) // strides[1]\n",
    "\n",
    "    # 计算卷积核的有效尺寸\n",
    "    effective_k_row = (ksizes[0] - 1) * rates[0] + 1\n",
    "    effective_k_col = (ksizes[1] - 1) * rates[1] + 1\n",
    "\n",
    "    # 计算需要的填充量\n",
    "    padding_rows = max(0, (out_rows - 1) * strides[0] + effective_k_row - rows)\n",
    "    padding_cols = max(0, (out_cols - 1) * strides[1] + effective_k_col - cols)\n",
    "\n",
    "    # 均匀填充\n",
    "    padding_top = int(padding_rows / 2.)\n",
    "    padding_left = int(padding_cols / 2.)\n",
    "    padding_bottom = padding_rows - padding_top\n",
    "    padding_right = padding_cols - padding_left\n",
    "\n",
    "    # 使用ZeroPad2d进行填充\n",
    "    paddings = (padding_left, padding_right, padding_top, padding_bottom)\n",
    "    images = torch.nn.ZeroPad2d(paddings)(images)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# 提取图像块的函数：从输入图像中提取小块用于后续的特征学习\n",
    "def extract_image_patches(images, ksizes, strides, rates, padding='same'):\n",
    "    assert len(images.size()) == 4\n",
    "    assert padding in ['same', 'valid']\n",
    "\n",
    "    if padding == 'same':\n",
    "        images = Apply_padding(images, ksizes, strides, rates)\n",
    "    elif padding == 'valid':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError(f'不支持的填充类型: {padding}. 仅支持 \"same\" 或 \"valid\".')\n",
    "\n",
    "    unfold = torch.nn.Unfold(kernel_size=ksizes, dilation=rates, padding=0, stride=strides)\n",
    "    patches = unfold(images)\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "# 将图像块重新转换回去：用于还原原始图像结构\n",
    "def reverse_patches(images, out_size, ksizes, strides, padding):\n",
    "    unfold = torch.nn.Fold(output_size=out_size, kernel_size=ksizes, dilation=1, padding=padding, stride=strides)\n",
    "    patches = unfold(images)\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "# 高效注意力机制：实现多头自注意力机制，减少维度并加速计算\n",
    "class EffAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., act_layer=Mish):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # 线性层将维度缩减一半\n",
    "        self.reduce = nn.Linear(dim, dim // 2, bias=qkv_bias)\n",
    "        # 生成查询、键和值的线性层\n",
    "        self.qkv = nn.Linear(dim // 2, dim // 2 * 3, bias=qkv_bias)\n",
    "        # 投影层\n",
    "        self.proj = nn.Linear(dim // 2, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reduce(x)\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # 将查询、键、值分割成多个块，提高计算效率\n",
    "        q_all = torch.split(q, math.ceil(N // 4), dim=-2)\n",
    "        k_all = torch.split(k, math.ceil(N // 4), dim=-2)\n",
    "        v_all = torch.split(v, math.ceil(N // 4), dim=-2)\n",
    "\n",
    "        output = []\n",
    "        for q, k, v in zip(q_all, k_all, v_all):\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale  # 计算注意力分数\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            trans_x = (attn @ v).transpose(1, 2)  # 应用注意力\n",
    "            output.append(trans_x)\n",
    "\n",
    "        # 将所有块合并为原始形状\n",
    "        x = torch.cat(output, dim=1)\n",
    "        x = x.reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# 卷积层：使用Mish作为激活函数，并支持BN + 激活函数的组合\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, nIn, nOut, kSize, stride, padding, dilation=(1, 1), groups=1, bn_acti=False, bias=False,\n",
    "                 act_layer=Mish):\n",
    "        super().__init__()\n",
    "        self.bn_acti = bn_acti\n",
    "        # 卷积层\n",
    "        self.conv = nn.Conv2d(nIn, nOut, kernel_size=kSize, stride=stride, padding=padding,\n",
    "                              dilation=dilation, groups=groups, bias=bias)\n",
    "        if self.bn_acti:\n",
    "            # 使用BN和Mish激活函数\n",
    "            self.bn_mish = BN_Mish(nOut, act_layer=act_layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        if self.bn_acti:\n",
    "            output = self.bn_mish(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# BatchNorm + Mish 组合模块\n",
    "class BN_Mish(nn.Module):\n",
    "    def __init__(self, nIn, act_layer=Mish):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(nIn, eps=1e-3)\n",
    "        self.acti = act_layer()\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.bn(input)\n",
    "        output = self.acti(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 深度可分离卷积：减少计算量并提高卷积层的效率\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size=3, stride=1, padding=1, dilation=1, bias=False, act_layer=Mish):\n",
    "        super().__init__()\n",
    "        # 深度卷积\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\n",
    "                                   groups=nin, bias=bias)\n",
    "        # 逐点卷积\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "        # 批归一化\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        # 激活函数\n",
    "        self.acti = act_layer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.acti(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Transformer Block：实现了注意力机制和多层感知机的结合\n",
    "class TransBlock(nn.Module):\n",
    "    def __init__(self, n_feat=32, dim=288, num_heads=8, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., act_layer=Mish, norm_layer=nn.LayerNorm):\n",
    "        super(TransBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.atten = EffAttention(self.dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                  attn_drop=attn_drop, proj_drop=drop, act_layer=act_layer)\n",
    "        self.norm1 = nn.LayerNorm(self.dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=dim // 4, act_layer=act_layer, drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 提取图像块\n",
    "        x = extract_image_patches(x, ksizes=[3, 3], strides=[1, 1], rates=[1, 1], padding='same')\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # 应用注意力和多层感知机\n",
    "        x = x + self.atten(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 长连接模块：通过分块卷积实现空间上的跨层连接\n",
    "class LongConnection(nn.Module):\n",
    "    def __init__(self, nIn, nOut, kSize, bn_acti=False, bias=False):\n",
    "        super().__init__()\n",
    "        self.bn_acti = bn_acti\n",
    "        self.dconv3x1 = nn.Conv2d(nIn, nIn // 2, (kSize, 1), 1, padding=(1, 0))  # 3x1卷积\n",
    "        self.dconv1x3 = nn.Conv2d(nIn // 2, nOut, (1, kSize), 1, padding=(0, 1))  # 1x3卷积\n",
    "        if self.bn_acti:\n",
    "            self.bn_mish = BN_Mish(nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.dconv3x1(input)\n",
    "        output = self.dconv1x3(output)\n",
    "        if self.bn_acti:\n",
    "            output = self.bn_mish(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 下采样模块：用于减少特征图的空间尺寸\n",
    "class DownSamplingBlock(nn.Module):\n",
    "    def __init__(self, nIn, nOut):\n",
    "        super().__init__()\n",
    "        self.nIn = nIn\n",
    "        self.nOut = nOut\n",
    "\n",
    "        if self.nIn < self.nOut:\n",
    "            nConv = nOut - nIn\n",
    "        else:\n",
    "            nConv = nOut\n",
    "\n",
    "        self.conv3x3 = Conv(nIn, nConv, kSize=3, stride=2, padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.bn_mish = BN_Mish(nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv3x3(input)\n",
    "        if self.nIn < self.nOut:\n",
    "            max_pool = self.max_pool(input)\n",
    "            output = torch.cat([output, max_pool], 1)\n",
    "        output = self.bn_mish(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 上采样模块：用于恢复特征图的空间尺寸\n",
    "class UpsampleingBlock(nn.Module):\n",
    "    def __init__(self, ninput, noutput):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
    "        self.relu = nn.ReLU6(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        output = self.bn(output)\n",
    "        output = self.relu(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 像素注意力模块：对每个像素应用注意力机制\n",
    "class PA(nn.Module):\n",
    "    def __init__(self, nf):\n",
    "        super(PA, self).__init__()\n",
    "        self.conv = nn.Conv2d(nf, nf, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        y = self.sigmoid(y)\n",
    "        out = torch.mul(x, y)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 通道注意力模块：通过全局平均池化和最大池化实现通道上的注意力机制\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "# 空间注意力模块：通过卷积操作在空间维度上实现注意力机制\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# CBAM模块：结合通道注意力和空间注意力\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.channel_attention(x) * x\n",
    "        out = self.spatial_attention(out) * out\n",
    "        return out\n",
    "\n",
    "\n",
    "# 优化的卷积块：结合深度可分离卷积、CBAM注意力机制和残差连接\n",
    "# OptimizedConvBlock\n",
    "class OCBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, use_attention=True):\n",
    "        super(OCBlock, self).__init__()\n",
    "        self.use_residual = (in_channels == out_channels and stride == 1)\n",
    "        self.conv = DepthwiseSeparableConv(in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "                                           act_layer=Mish)\n",
    "        self.bn_mish = BN_Mish(out_channels)\n",
    "        self.attention = CBAM(out_channels) if use_attention else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv(x)\n",
    "        out = self.bn_mish(out)\n",
    "\n",
    "        # 加入CBAM注意力机制\n",
    "        if self.attention is not None:\n",
    "            out = self.attention(out)\n",
    "\n",
    "        # 残差连接\n",
    "        if self.use_residual:\n",
    "            out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "# 动态卷积：动态生成卷积核的权重\n",
    "class DynamicConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, act_layer=Mish):\n",
    "        super(DynamicConv, self).__init__()\n",
    "        self.dynamic_weights = nn.Parameter(torch.Tensor(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        nn.init.kaiming_uniform_(self.dynamic_weights, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.dynamic_weights\n",
    "        out = F.conv2d(x, weight, self.bias, stride=1, padding=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 主网络：集成深度可分离卷积、下采样、上采样、注意力机制、残差连接等模块\n",
    "class self_net(nn.Module):\n",
    "    def __init__(self, classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # 初始化卷积块\n",
    "        self.init_conv = nn.Sequential(\n",
    "            DepthwiseSeparableConv(3, 32, 3, 1, padding=1, act_layer=Mish),  # 深度可分离卷积层\n",
    "            DepthwiseSeparableConv(32, 32, 3, 1, padding=1, act_layer=Mish),  # 深度可分离卷积层\n",
    "            DepthwiseSeparableConv(32, 32, 3, 2, padding=1, act_layer=Mish),  # 深度可分离卷积层，步长为2\n",
    "        )\n",
    "\n",
    "        # 批归一化 + Mish 激活层\n",
    "        self.bn_mish_1 = BN_Mish(32)  # 第一个 BN + Mish 层\n",
    "        self.bn_mish_2 = BN_Mish(64)  # 第二个 BN + Mish 层\n",
    "        self.bn_mish_3 = BN_Mish(128) # 第三个 BN + Mish 层\n",
    "        self.bn_mish_4 = BN_Mish(32)  # 第四个 BN + Mish 层\n",
    "        self.bn_mish_5 = BN_Mish(16)  # 第五个 BN + Mish 层\n",
    "        self.bn_mish_6 = BN_Mish(16)  # 第六个 BN + Mish 层\n",
    "        self.bn_mish_7 = BN_Mish(16)  # 第七个 BN + Mish 层\n",
    "\n",
    "        # 下采样块\n",
    "        self.downsample_1 = DownSamplingBlock(32, 64)   # 第一次下采样\n",
    "        self.downsample_2 = DownSamplingBlock(64, 128)  # 第二次下采样\n",
    "        self.downsample_3 = DownSamplingBlock(128, 32)  # 第三次下采样\n",
    "\n",
    "        # 优化卷积块\n",
    "        self.Block_1 = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.Block_1.add_module(f\"OCBlock_1_{i}\", OCBlock(64, 64, use_attention=True))  # 第一个优化卷积块\n",
    "\n",
    "        self.Block_2 = nn.Sequential()\n",
    "        for i in range(12):\n",
    "            self.Block_2.add_module(f\"OCBlock_2_{i}\", OCBlock(128, 128, use_attention=True))  # 第二个优化卷积块\n",
    "\n",
    "        self.Block_3 = nn.Sequential()\n",
    "        for i in range(12):\n",
    "            self.Block_3.add_module(f\"OCBlock_3_{i}\", OCBlock(32, 32, use_attention=True))  # 第三个优化卷积块\n",
    "\n",
    "        # Transformer 块\n",
    "        self.transformer1 = TransBlock(dim=288)  # Transformer 处理块\n",
    "        self.transformer2 = TransBlock(dim=288)  # Transformer 处理块\n",
    "        self.transformer3 = TransBlock(dim=144)  # Transformer 处理块\n",
    "\n",
    "        # 上采样块和优化卷积块\n",
    "        self.Block_4 = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.Block_4.add_module(f\"OCBlock_4_{i}\", OCBlock(32, 32, use_attention=True))  # 第四个优化卷积块\n",
    "        self.upsample_1 = UpsampleingBlock(32, 16)  # 第一次上采样\n",
    "        self.Block_5 = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.Block_5.add_module(f\"OCBlock_5_{i}\", OCBlock(16, 16, use_attention=True))  # 第五个优化卷积块\n",
    "        self.upsample_2 = UpsampleingBlock(16, 16)  # 第二次上采样\n",
    "        self.Block_6 = nn.Sequential()\n",
    "        for i in range(3):\n",
    "            self.Block_6.add_module(f\"OCBlock_6_{i}\", OCBlock(16, 16, use_attention=True))  # 第六个优化卷积块\n",
    "        self.upsample_3 = UpsampleingBlock(16, 16)  # 第三次上采样\n",
    "\n",
    "        # 注意力机制\n",
    "        self.PA = PA(16)  # 通道注意力模块\n",
    "\n",
    "        # 长连接（用于跳跃连接）\n",
    "        self.LC1 = LongConnection(64, 16, 3)   # 第一长连接\n",
    "        self.LC2 = LongConnection(128, 16, 3)  # 第二长连接\n",
    "        self.LC3 = LongConnection(32, 32, 3)   # 第三长连接\n",
    "\n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(Conv(16, classes, 1, 1, padding=0))  # 最后的分类层\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = F.interpolate(input, size=(208, 208), mode='bilinear', align_corners=False)\n",
    "        output0 = self.init_conv(input)\n",
    "        output0 = self.bn_mish_1(output0)\n",
    "\n",
    "        b, c, h, w = output0.shape\n",
    "        output1_0 = self.transformer1(output0)\n",
    "        output1_0 = output1_0.permute(0, 2, 1)\n",
    "        output1_0 = reverse_patches(output1_0, (h, w), (3, 3), 1, 1)\n",
    "\n",
    "        output1_0 = self.downsample_1(output0)\n",
    "        output1 = self.Block_1(output1_0)\n",
    "        output1 = self.bn_mish_2(output1)\n",
    "\n",
    "        output2_0 = self.downsample_2(output1)\n",
    "        output2 = self.Block_2(output2_0)\n",
    "        output2 = self.bn_mish_3(output2)\n",
    "\n",
    "        output3_0 = self.downsample_3(output2)\n",
    "        output3 = self.Block_3(output3_0)\n",
    "        output3 = self.bn_mish_4(output3)\n",
    "\n",
    "        b, c, h, w = output3.shape\n",
    "        output4 = self.transformer2(output3)\n",
    "        output4 = output4.permute(0, 2, 1)\n",
    "        output4 = reverse_patches(output4, (h, w), (3, 3), 1, 1)\n",
    "\n",
    "        output4 = self.Block_4(output4)\n",
    "        output4 = self.upsample_1(output4 + self.LC3(output3))\n",
    "        output4 = self.bn_mish_5(output4)\n",
    "  \n",
    "        output5 = self.Block_5(output5)\n",
    "        output5 = self.upsample_2(output5 + self.LC2(output2))\n",
    "        output5 = self.bn_mish_6(output5)\n",
    "\n",
    "        output6 = self.Block_6(output6)\n",
    "        output6 = self.upsample_3(output6 + self.LC1(output1))\n",
    "        output6 = self.PA(output6)\n",
    "        output6 = self.bn_mish_7(output6)\n",
    "\n",
    "        b, c, h, w = output6.shape\n",
    "        output6 = self.transformer3(output6)\n",
    "        output6 = output6.permute(0, 2, 1)\n",
    "        output6 = reverse_patches(output6, (h, w), (3, 3), 1, 1)\n",
    "\n",
    "        out = F.interpolate(output6, size=(200, 200), mode='bilinear', align_corners=False)\n",
    "        out = self.classifier(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f82e7b-f0d7-44d7-9b85-ce3d0b28a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型的总参数量为: 847111\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型并计算参数量\n",
    "n_class = 4\n",
    "model = self_net(n_class)\n",
    "\n",
    "# 计算总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"模型的总参数量为: {total_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
